多层前馈网络
=================================

在前面我们已经介绍了人脑的神经细胞， 以及由此抽象出来的人工神经网络模型， 并且介绍了自动训练模型参数 （即根据具体问题确定神经网络中边的权重） 的方法。 人工神经元相互连接可以构成很多不同拓扑结构的神经网络。 本节将介绍一种使用较为普遍的多层前馈网络， 这也是深度学习中普遍使用的网络结构。

多层前馈网络
----------------------------------

人工神经网络可以用图表示， 神经元是图的节点， 神经元之间的联系是图中的有向边。 神经元不同的连接方式就会形成不同拓扑结构的神经网络。 目前神经网络的自动学习几乎只局限于神经网络边连接的权重， 不会去自动学习网络拓扑结构。 而不同拓扑结构的网络适用于不同类型的问题， 设计神经网络结构这部分工作还需要设计者的参与。

如同从人脑神经细胞工作原理中得来灵感设计人工神经元， 在设计人工神经元连接方式时， 也值得参考人脑中神经细胞的连接方式。 我们知道负责高层信息处理的脑细胞集中在大脑皮层。 人脑大量的沟回增大了皮层表面积， 也使得人脑有更强的信息处理能力。 如果细看皮层， 可以发现其中的神经细胞的排列是很有规律的。 在网络上能找到大脑皮层的示意图 （如维基百科的这一页面 `<http://zh.wikipedia.org/wiki/大脑皮层>`_ ）， 可以看到皮层中的神经细胞是分层排列的， 如同汉堡或三明治。 同时， 树突和轴突也是有方向性的， 即电信号都是由内层神经元传向外层神经元的。 

与这种人脑皮层神经细胞连接方式类似的人工神经网络就是多层前馈网络。 它的示意图如下图所示：

.. graphviz::

    digraph grph{
    rankdir=LR;
    node [shape = none];
    x1;
    x2;
    //xx[label="1"];
    y;
    node [shape = circle];
    {rank=same;
    h11[label="a1"] h12[label="a2"] h13[label="a3"]; 
    //h14[label="1"];
    };
    x1 -> h11;x2->h11;//xx->h11;
    x1 -> h12;x2->h12;//xx->h12;
    x1 -> h13;x2->h13;//xx->h13;
    //h11->h14 [style=invis]

    h21[label="b1"] h22[label="b2"] h23[label="b3"];
    //h24[label="1"];

    h11->h21;h12->h21;h13->h21;
    h11->h22;h12->h22;h13->h22;
    h11->h23;h12->h23;h13->h23;
    //h14->h21;h14->h22;h14->h23;
    h21->y;
    h22->y;h23->y;//h24->y;
    }


信号由左至右向前单向传播， 每一列神经元构成一层， 因此被称为多层前馈网络。 网络相邻两层的任意神经元之间都有连接，此外没有层间连接或者跨越多层的连接。 网络的输入输出数目和形式根据具体问题指定， 网络层数和每层神经元个数也是需要人为指定的。



一个多层前馈网络将单个的、 功能有限的人工神经元组织成了一个整体的计算单元， 类似于计算机中的一个函数或一段程序。 但它与我们熟悉的计算机程序又有所不同。 它的计算没有迭代， 并且是并行的。 这很类似人脑进行视觉信息处理的情况。 人脑的视觉皮层 （visual cortex） 负责处理视觉信息， 视网膜得到一个影像后， 大脑并不会一个“像素” 一个像素地进行扫描并处理， 然后分辨看到的到底是什么， 而是通过一组神经神经细胞， 同时对传回的信号进行并行处理。 这也是为什么长于串行计算的CPU不适合处理视频信息， 电脑的图像处理需要长于并行计算的GPU来完成。 有意思的是这也适用于人工神经网络， 使用GPU来模拟人工神经网络要比使用CPU高效得多。


让我们回到上图所示的神经网。 在数学上， 它可以写成嵌套函数。 图中有三列箭头， 分别将前一层的数据变为后一层的数据， 对应着三个函数， 可用 :math:`f_1` 、 :math:`f_2` 、 :math:`f_3` 表示：

.. math::

    \mathbf{a}=f_1(\mathbf{x})=s(\mathbf{x}\mathbf{w}_1+\mathbf{b}_1)

.. math::

    \mathbf{b}=f_2(\mathbf{a})=s(\mathbf{a}\mathbf{w}_2+\mathbf{b}_2)

.. math::

    \mathbf{y}=f_3(\mathbf{b})=s(\mathbf{b}\mathbf{w}_3+\mathbf{b}_3)

那么整个函数就表示为：

.. math::

    \mathbf{y}=f_1(f_2(f_3(\mathbf{x})))


后向传播算法计算梯度
----------------------------------------

至此我们已经将多层神经网络的结构介绍清楚了。 其参数学习方法可以使用上节介绍的梯度下降算法， 也就是要求这样的偏导数：

.. math::

    \left.\frac{\partial{\text{Loss}}}{\partial{\mathbf{w}}}\right|_{\mathbf{w}}

由于多层前馈网络对应的表达式是一个嵌套函数， 并且函数的输入于输出都可能是向量， 因此想要求出损失函数对于其中每个函数参数的导数， 其方法并非那么直观。 最初（几十年前）， 学者甚至绕过梯度下降算法， 使用其它方法确定参数。 直到计算此梯度的一般方法被提出， 还被冠以一个单独的名字——“后向传播算法”。 该算法使得可以使用梯度下降算法确定参数， 因此更有效， 被以里程碑的形式写入历史。 这里我们不想细致介绍计算多层前馈神经网络中的每一个步骤， 罗列每一个公式， 这些内容读者在任何一本教材甚至许多网络文章中都可以找到。 在此我们知识想说明后向传播算法所使用的数学原理， 其实也就是每本微积分教材中会提到的计算导数的链式法则。

让我们回忆一下如何计算 :math:`y=(2x+1)^2` 的导数。 我们学到的方法是先将 :math:`2x+1` 看作一个整体 :math:`z=2x+1` ， 然后分别计算 :math:`z^2` 和 :math:`2x+1` 的导数， 然后相乘就可以了， 即

.. math::

    \frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}

上面公式的物理意味就更加明显了。 我们想要计算 :math:`\frac{dy}{dx}` 即 :math:`x` 变化一个微小量后， :math:`y` 的变化有多大。 我们知道 :math:`x` 会影响 :math:`z` ， 然后 :math:`z` 再影响 :math:`y` 。 那么我们就可以先计算 :math:`x` 的变化对 :math:`z` 的影响有多大， 然后计算 :math:`z` 的变化对 :math:`y` 的影响有多大， 两者合起来（相乘）就可以得到:math:`x` 变化一个微小量后， :math:`y` 的变化有多大。 升值更为直观的解释是直接将上式右边理解为两个分式相乘， 消去相同项后就得到右式（如果将公式中的 “ :math:`d` ” 换成 “ :math:`\Delta` ”， 这样说就更为严谨了 ）。 最早使用链式法则可能可以追踪到莱布尼茨。


回到前述神经网络中， 如果我们要计算 :math:`\frac{\partial\text{loss}}{\partial \mathbf{w}_2}` ，实际上只需要将其分解为：

.. math::

    \frac{\partial\text{loss}}{\partial \mathbf{b}} \frac{\partial \mathbf{b}}{\partial \mathbf{w}_2}

第二项比较容易， 根据函数 :math:`f_2` 的表达式就可以求得。 而第一项是另一个关于变量 :math:`b` 梯度 的梯度， 也可以被分解：

.. math::

    \frac{\partial\text{loss}}{\partial \mathbf{b}} = \frac{\partial\text{loss}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{b}}

以上第二项同样比较容易计算， 根据函数 :math:`f_3` 就可求得。 而第一项可以根据具体损失函数求得其梯度。

这就是梯度计算的步骤， 在图中看的话就是， 需要先计算关于靠右变量的梯度， 才能计算出关于靠左变量的梯度， 例如要想求得关于 :math:`\mathbf{y}` 的梯度才能求关于 :math:`\mathbf{b}` 的梯度， 最后才能求得关于 :math:`\mathbf{w}_2` 的梯度。 计算各个变量的值是从做到右计算的， 而计算梯度是从右向左计算的， 这就是后向传播算法名字的来历。

以上我们看到， 计算梯度所使用的后向传播算法， 也就是微积分中最为普通的链式法则的具体应用。 可见数学对于计算机科学也是非常重要的。 
