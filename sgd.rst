参数学习
==============================

上一节我们介绍了人工神经网络如何脱胎于生物实体， 成为一个纯粹的数学模型。 这一节我们会看到这个数学模型又是如何从一个理论模型，变为工程上可实际运用的工具。

这一节所考虑的问题是， 在神经网络结构确定之后，如何确定其中的参数， 即上一节中针对单个神经元的 :math:`\mathbf{w}` 以及 :math:`b` 。 为了描述的简洁， 在本节中使用 :math:`\mathbf{w}` 表示所有需要确定的参数。

模型的评价
----------------------------------------------------

首先我们要讨论如何评价模型参数的好坏， 否则讨论“如何选择好的参数”这个问题就是没有意义的。

一个好的神经网络， 对于给定的输入， 能够得到设计者期望的输出。 设输入为 :math:`\mathbf{x}` ， 输出为 :math:`\mathbf{y}` ， 我们期望的输出为 :math:`\mathbf{y}^*` ，可定义实际输出与期望输出的差别作为评价神经网络好坏的指标。 例如可以将两个向量的距离的平方定义为这个差别

.. math::

    d(\mathbf{y}^*,\mathbf{y})=\frac{1}{2}\|\mathbf{y}^*-\mathbf{y}\|^2


只考察单个样本还不够充分，通常需要考察一个样本的集合，例如有 :math:`\{(\mathbf{x}_i,\mathbf{y}^*_i)|i=1\cdots N \}` ， 定义损失函数为：

.. math::

    \text{loss}(\mathbf{w})=\sum_{i=1}^{N}{d(\mathbf{y}^*_i,\mathbf{y}_i)}

这样的用于评价参数好坏的样本集在机器学习中称为 **测试集** 。

有监督学习
----------------------------------------------------

既然损失函数可以用来评价模型的好坏，那么让损失函数的值最小的那组参数就应该是最好的参数：

.. math::

    \hat{\mathbf{w}}=\arg\min_{\mathbf{w}}{\text{loss}(\mathbf{w})}

在数学上， 这是一个无约束优化问题， 即调整一组变量使得某个表达式最小（或最大），而对所调整的变量的取值没有限制。

但这里还有个问题， 我们是否可以用测试集上的损失函数来调整参数呢？ 答案是否定的， 这会产生严重的过拟合， 即由于参数的学习是依赖于某个给定的样本及标准输出的集合， 那么学习得到的模型就很有可能在这一集合上的损失很低， 但在之外的集合上的损失就会偏高。 那么如果用训练所用集合来评价模型， 分数就会偏高。 还有一个形象的比方是， 考试是用来检验学生对知识的掌握， 学生需要在考试之外也能运用知识。 如果学生在学习时过分针对考试（甚至是知道考试的题目）， 那么可以想象这种应试的学习并不能保证学生在考试之外能够真正运用知识。

以上讨论了一大堆， 解决方法其实并不复杂，即使用两个集合，一个是测试集， 一个是训练集， 参数的学习只针对训练集， 找到使训练集损失尽量小的参数， 然后在测试集上测试该组参数面对训练集之外的样本的表现。


梯度下降法
-------------------------------------------

.. topic:: 梯度下降算法

    1. 初始化参数 :math:`\mathbf{w}_0` 、 :math:`t=0`
    2. 步数 :math:`t\leftarrow t+1`
    3. 计算梯度 :math:`\nabla \mathbf{w} =\left.\frac{\partial \text{loss}}{\partial \mathbf{w}}\right|_{\mathbf{w}_{t-1}}`
    4. 更新参数 :math:`\mathbf{w}_t \leftarrow \mathbf{w}_{t-1}-\eta\nabla \mathbf{w}`
    5. 如果收敛，结束并输出 :math:`\mathbf{w}_t` ，否则转到步骤2

这里主要的计算是第3步，计算梯度。这要求损失函数对于参数可导。

是一种下山法（或者叫爬山法）， 也可以称作一种贪心的搜索方法。

虽然如此简单， 但它却是最常见的参数优化方法。

